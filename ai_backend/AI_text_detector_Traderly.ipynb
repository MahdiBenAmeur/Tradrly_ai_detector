{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**this notebook is created under the traderly challange of creating an ai model capable of detecting if a text is ai generated or humanly written**"
      ],
      "metadata": {
        "id": "HAiNYcXur2kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the approach i'll be taking is to find a large dataset of both ai and human written text , then finetune the NLP model BERT on it\n",
        "for more options ill be doing this using both the base and large model and comparing accuracy and speed of them just to give prespective for the Traderly team to decide what to chose"
      ],
      "metadata": {
        "id": "npYZLbaOsabs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#liberaries"
      ],
      "metadata": {
        "id": "xpBJyH2Dt78Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fVUmb3_M21W",
        "outputId": "d44e43d2-4966-4248-fd84-d23c4a55812b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uI_N-QqLt_Tc",
        "outputId": "c68e4ce8-30df-4d72-b961-dd72c14c58d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer ,DataCollatorWithPadding ,  TrainingArguments , AutoModelForSequenceClassification\n",
        "\n"
      ],
      "metadata": {
        "id": "0vB_ZRo4uB_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset\n",
        "first lets get a dataset !\n",
        "after a bit of reaserch i found multiple goodones , for now lets stick with this one"
      ],
      "metadata": {
        "id": "txCbxAICtcxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ds = load_dataset(\"andythetechnerd03/AI-human-text\")\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRMBQAt3sYWr",
        "outputId": "c2a8043f-74b5-4115-a83a-fabd5db4b627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'generated'],\n",
              "        num_rows: 462873\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'generated'],\n",
              "        num_rows: 24362\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets see some exemples"
      ],
      "metadata": {
        "id": "6462H4N0uPIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_exemples = 5\n",
        "for i in range(nb_exemples):\n",
        "  print(\"--------------------------------------------------------------------------\")\n",
        "  print(f\"is ai : {ds['train'][i]['generated']==0} \")\n",
        "\n",
        "  print(f\"text  : {ds['train'][i]['text'][:200]}\") # text too long to be shown so 200 car is enaugh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1KqOgCft3t7",
        "outputId": "a7d3f218-0f97-42e6-b7f4-307509d3a2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------\n",
            "is ai : True \n",
            "text  : studies have been proven that people are starting to not drive cars as much americans are buying fewer cars and getting fewer licenses there are several advantages to not driving cars everywhere benef\n",
            "--------------------------------------------------------------------------\n",
            "is ai : False \n",
            "text  : i disagree with you ih twenty years the humber of cars ih use will be the same or even greater that it is today there are a humber of reasons why this is likely to be the case firstly the humber of pe\n",
            "--------------------------------------------------------------------------\n",
            "is ai : False \n",
            "text  : hey yall today were gonna talk about this cool idea that winston churchill had about success he said that its all about going through failure without losing your enthusiasm lime whoa right let me tell\n",
            "--------------------------------------------------------------------------\n",
            "is ai : True \n",
            "text  : im going to start explaining and giving my opinion technology is one of the main reasons why the world is changing and accommodating with the necessities that the person have for example we live in a \n",
            "--------------------------------------------------------------------------\n",
            "is ai : True \n",
            "text  : limiting car usage can be very helpful not only to our environment but to our culture our ozone is slowly getting worse and worse every year a heavy contributor to the decay of the ozone layer is the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets split the data and create our data loaders for later use"
      ],
      "metadata": {
        "id": "c7DzMEepvt56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.rename_column('generated', 'labels')\n"
      ],
      "metadata": {
        "id": "g0JKdgLs7nJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ds['train']\n",
        "test_data = ds['test']\n"
      ],
      "metadata": {
        "id": "Q4pVdnuiuXKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n",
        "now we will get the weights of the base models , prepare them for finetunning and train them on the new data"
      ],
      "metadata": {
        "id": "fVvqZLTQwH3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "t67GtNP0wZCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets tokenize the dataset!"
      ],
      "metadata": {
        "id": "xyYdVNi4yhRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization_function(row):\n",
        "  return tokenizer(row['text'] , truncation=True)\n",
        "\n",
        "\n",
        "tokenized_train_data = train_data.map(tokenization_function, batched=True)\n",
        "tokenized_test_data = test_data.map(tokenization_function, batched=True)\n",
        "#exemple\n",
        "tokenized_train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tatYq6zzwgla",
        "outputId": "a2b2a588-838f-40f0-9732-b0b921cf8aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'studies have been proven that people are starting to not drive cars as much americans are buying fewer cars and getting fewer licenses there are several advantages to not driving cars everywhere beneficial implications for carbon emissions the environment and improves safetyvaubans streets in germany are completely car free people who do not drive cars any more and ride bikes walks or rides the tram are said to be happier this way a mother of two walks and rides her bikes everywhere with her kids and does not feel as tense and stressed as she did when she was drivingalso a huge advantage is it drastically reduces greenhouse gas emissions from tailpipes diesel fuel was banned in france since tax policies favored diesel over gasoline delivery companies complained of lost revenue but exceptions were made plugin cars hybrids and cars carrying three or more passengersin paris there were days of near record pollution so they enforced a partial driving ban to clear the air of the city april 2013 the number of miles driven per person was nearly 9 percent below average and equal to the country in january 1995 the rate of car ownership per household and per person came down two to three years before the downturn michael siva stated new york has a new bike sharing program and the skyrocketing bridges and tunnel tolls reflect people to want to not drive cars anymorea study last year found that driving by young people decreased 23 percent between 2001 and 2009 just shows you that the beneficial implications environment and easier way of saving money does make people want to start biking walking using a tram etc those examples i gave you are a few advantages of limiting car use',\n",
              " 'labels': 0,\n",
              " 'input_ids': [101,\n",
              "  2913,\n",
              "  2031,\n",
              "  2042,\n",
              "  10003,\n",
              "  2008,\n",
              "  2111,\n",
              "  2024,\n",
              "  3225,\n",
              "  2000,\n",
              "  2025,\n",
              "  3298,\n",
              "  3765,\n",
              "  2004,\n",
              "  2172,\n",
              "  4841,\n",
              "  2024,\n",
              "  9343,\n",
              "  8491,\n",
              "  3765,\n",
              "  1998,\n",
              "  2893,\n",
              "  8491,\n",
              "  15943,\n",
              "  2045,\n",
              "  2024,\n",
              "  2195,\n",
              "  12637,\n",
              "  2000,\n",
              "  2025,\n",
              "  4439,\n",
              "  3765,\n",
              "  7249,\n",
              "  15189,\n",
              "  13494,\n",
              "  2005,\n",
              "  6351,\n",
              "  11768,\n",
              "  1996,\n",
              "  4044,\n",
              "  1998,\n",
              "  24840,\n",
              "  3808,\n",
              "  3567,\n",
              "  19761,\n",
              "  3619,\n",
              "  4534,\n",
              "  1999,\n",
              "  2762,\n",
              "  2024,\n",
              "  3294,\n",
              "  2482,\n",
              "  2489,\n",
              "  2111,\n",
              "  2040,\n",
              "  2079,\n",
              "  2025,\n",
              "  3298,\n",
              "  3765,\n",
              "  2151,\n",
              "  2062,\n",
              "  1998,\n",
              "  4536,\n",
              "  18105,\n",
              "  7365,\n",
              "  2030,\n",
              "  12271,\n",
              "  1996,\n",
              "  12517,\n",
              "  2024,\n",
              "  2056,\n",
              "  2000,\n",
              "  2022,\n",
              "  19366,\n",
              "  2023,\n",
              "  2126,\n",
              "  1037,\n",
              "  2388,\n",
              "  1997,\n",
              "  2048,\n",
              "  7365,\n",
              "  1998,\n",
              "  12271,\n",
              "  2014,\n",
              "  18105,\n",
              "  7249,\n",
              "  2007,\n",
              "  2014,\n",
              "  4268,\n",
              "  1998,\n",
              "  2515,\n",
              "  2025,\n",
              "  2514,\n",
              "  2004,\n",
              "  9049,\n",
              "  1998,\n",
              "  13233,\n",
              "  2004,\n",
              "  2016,\n",
              "  2106,\n",
              "  2043,\n",
              "  2016,\n",
              "  2001,\n",
              "  4439,\n",
              "  9777,\n",
              "  2080,\n",
              "  1037,\n",
              "  4121,\n",
              "  5056,\n",
              "  2003,\n",
              "  2009,\n",
              "  21040,\n",
              "  13416,\n",
              "  16635,\n",
              "  3806,\n",
              "  11768,\n",
              "  2013,\n",
              "  5725,\n",
              "  24548,\n",
              "  2015,\n",
              "  7937,\n",
              "  4762,\n",
              "  2001,\n",
              "  7917,\n",
              "  1999,\n",
              "  2605,\n",
              "  2144,\n",
              "  4171,\n",
              "  6043,\n",
              "  12287,\n",
              "  7937,\n",
              "  2058,\n",
              "  13753,\n",
              "  6959,\n",
              "  3316,\n",
              "  10865,\n",
              "  1997,\n",
              "  2439,\n",
              "  6599,\n",
              "  2021,\n",
              "  11790,\n",
              "  2020,\n",
              "  2081,\n",
              "  13354,\n",
              "  2378,\n",
              "  3765,\n",
              "  23376,\n",
              "  1998,\n",
              "  3765,\n",
              "  4755,\n",
              "  2093,\n",
              "  2030,\n",
              "  2062,\n",
              "  5467,\n",
              "  2378,\n",
              "  3000,\n",
              "  2045,\n",
              "  2020,\n",
              "  2420,\n",
              "  1997,\n",
              "  2379,\n",
              "  2501,\n",
              "  10796,\n",
              "  2061,\n",
              "  2027,\n",
              "  16348,\n",
              "  1037,\n",
              "  7704,\n",
              "  4439,\n",
              "  7221,\n",
              "  2000,\n",
              "  3154,\n",
              "  1996,\n",
              "  2250,\n",
              "  1997,\n",
              "  1996,\n",
              "  2103,\n",
              "  2258,\n",
              "  2286,\n",
              "  1996,\n",
              "  2193,\n",
              "  1997,\n",
              "  2661,\n",
              "  5533,\n",
              "  2566,\n",
              "  2711,\n",
              "  2001,\n",
              "  3053,\n",
              "  1023,\n",
              "  3867,\n",
              "  2917,\n",
              "  2779,\n",
              "  1998,\n",
              "  5020,\n",
              "  2000,\n",
              "  1996,\n",
              "  2406,\n",
              "  1999,\n",
              "  2254,\n",
              "  2786,\n",
              "  1996,\n",
              "  3446,\n",
              "  1997,\n",
              "  2482,\n",
              "  6095,\n",
              "  2566,\n",
              "  4398,\n",
              "  1998,\n",
              "  2566,\n",
              "  2711,\n",
              "  2234,\n",
              "  2091,\n",
              "  2048,\n",
              "  2000,\n",
              "  2093,\n",
              "  2086,\n",
              "  2077,\n",
              "  1996,\n",
              "  2091,\n",
              "  22299,\n",
              "  2745,\n",
              "  9033,\n",
              "  3567,\n",
              "  3090,\n",
              "  2047,\n",
              "  2259,\n",
              "  2038,\n",
              "  1037,\n",
              "  2047,\n",
              "  7997,\n",
              "  6631,\n",
              "  2565,\n",
              "  1998,\n",
              "  1996,\n",
              "  3712,\n",
              "  16901,\n",
              "  20624,\n",
              "  3070,\n",
              "  7346,\n",
              "  1998,\n",
              "  5234,\n",
              "  9565,\n",
              "  2015,\n",
              "  8339,\n",
              "  2111,\n",
              "  2000,\n",
              "  2215,\n",
              "  2000,\n",
              "  2025,\n",
              "  3298,\n",
              "  3765,\n",
              "  4902,\n",
              "  2050,\n",
              "  2817,\n",
              "  2197,\n",
              "  2095,\n",
              "  2179,\n",
              "  2008,\n",
              "  4439,\n",
              "  2011,\n",
              "  2402,\n",
              "  2111,\n",
              "  10548,\n",
              "  2603,\n",
              "  3867,\n",
              "  2090,\n",
              "  2541,\n",
              "  1998,\n",
              "  2268,\n",
              "  2074,\n",
              "  3065,\n",
              "  2017,\n",
              "  2008,\n",
              "  1996,\n",
              "  15189,\n",
              "  13494,\n",
              "  4044,\n",
              "  1998,\n",
              "  6082,\n",
              "  2126,\n",
              "  1997,\n",
              "  7494,\n",
              "  2769,\n",
              "  2515,\n",
              "  2191,\n",
              "  2111,\n",
              "  2215,\n",
              "  2000,\n",
              "  2707,\n",
              "  28899,\n",
              "  3788,\n",
              "  2478,\n",
              "  1037,\n",
              "  12517,\n",
              "  4385,\n",
              "  2216,\n",
              "  4973,\n",
              "  1045,\n",
              "  2435,\n",
              "  2017,\n",
              "  2024,\n",
              "  1037,\n",
              "  2261,\n",
              "  12637,\n",
              "  1997,\n",
              "  14879,\n",
              "  2482,\n",
              "  2224,\n",
              "  102],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we dont have enaugh time so imma train it only on 20 k , randomized ofc so we dont get unballanced data"
      ],
      "metadata": {
        "id": "Ewfn2-R7rQ75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Find indices of each label\n",
        "label_0_indices = [i for i, row in enumerate(tokenized_train_data) if row['labels'] == 0]\n",
        "label_1_indices = [i for i, row in enumerate(tokenized_train_data) if row['labels'] == 1]\n",
        "\n",
        "# 2. How many from each?\n",
        "min_count = min(len(label_0_indices), len(label_1_indices), 10000)\n",
        "\n",
        "# 3. Randomly pick\n",
        "import random\n",
        "sampled_0_indices = random.sample(label_0_indices, min_count)\n",
        "sampled_1_indices = random.sample(label_1_indices, min_count)\n",
        "\n",
        "# 4. Combine and shuffle\n",
        "all_indices = sampled_0_indices + sampled_1_indices\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "# 5. Select\n",
        "small_tokenized_train_data = tokenized_train_data.select(all_indices)\n",
        "\n",
        "# now small_tokenized_train_data is your 20k balanced dataset\n"
      ],
      "metadata": {
        "id": "6MWw7HdF-37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets prepare the Trainer!"
      ],
      "metadata": {
        "id": "8xx7GPcj2yjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name , num_labels=2)\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=5,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=1e-4,\n",
        "    max_grad_norm=0.01,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "3tfYuH0sz2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ee8b7a-5abf-45d7-e02c-88a4a7d84f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=small_tokenized_train_data,\n",
        "    eval_dataset=tokenized_test_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "bfcUbFid14yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78bd6e1e-d91f-4cdc-8bf4-88927c37ee0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-3c59f2d985c1>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and now we can train!"
      ],
      "metadata": {
        "id": "qq1wJz9v6QHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CjjnKjNW6PkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "80ad1228-8cbf-4973-b007-ff9c611fb179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 54:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.170100</td>\n",
              "      <td>0.402464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.049300</td>\n",
              "      <td>0.031087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.033700</td>\n",
              "      <td>0.037467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.048024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.094074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3125, training_loss=0.04343643413424492, metrics={'train_runtime': 3294.1141, 'train_samples_per_second': 30.357, 'train_steps_per_second': 0.949, 'total_flos': 2.6311105536e+16, 'train_loss': 0.04343643413424492, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the loss seems greatt , but lets test its accuracy on unseen data"
      ],
      "metadata": {
        "id": "BLhuLqoS8NFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = np.mean(predictions == labels)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "eval_pred = trainer.predict(tokenized_test_data)\n",
        "\n",
        "metrics = compute_metrics((eval_pred.predictions, eval_pred.label_ids))\n",
        "\n",
        "print(metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "aK85xDHV8Fr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f050f44-1d2e-447b-bdd9-32f479aa642a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': np.float64(0.940316886955094)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now thats a great accuracy and only on 20 k rows of data , givin enaugh time we will achieve better resultts, lets save the model"
      ],
      "metadata": {
        "id": "oqR7-VnX8k0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_path = \"my_ai_detection_model\"\n",
        "\n",
        "\n",
        "trainer.save_model(model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
        "\n",
        "\n",
        "# Example usage after reloading\n",
        "def predict_ai_generated(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "    return predicted_class\n",
        "\n",
        "example_text = \"Once upon a time, in a small village tucked between two mountains, there lived a curious young boy named Leo. Every day after his chores, Leo would wander into the woods, dreaming of finding something extraordinary.\\n\\nOne afternoon, as he followed a stream deeper than he ever had before, he stumbled upon a hidden cave. Inside, glowing crystals lit up the walls, and in the center sat an ancient book. The book spoke of a forgotten treasure that could bring great prosperity to the village  but only if found with a pure heart.\\n\\nDetermined, Leo set off on a journey across forests, rivers, and deserts. Along the way, he helped strangers, solved riddles, and resisted temptations. In the end, Leo didnt find gold or jewels  the real treasure was the friendships he built and the wisdom he gained.\\n\\nWhen he returned home, the village celebrated him not for bringing riches, but for bringing back stories, hope, and courage they had long forgotten.\\n\\nAnd so, Leos name lived on, not in legend for wealth, but for kindness\"\n",
        "prediction = predict_ai_generated(example_text)\n",
        "print(f\"Prediction for the example text: {prediction}\") # Output: 0 or 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gj908p6tzFA",
        "outputId": "781404d7-b80c-46ed-fd02-d14fcbb364d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the example text: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: transform the model to my drive\n",
        "\n",
        "# Save the model to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/my_ai_detection_model\"  # Choose a path in your Google Drive\n",
        "\n",
        "trainer.save_model(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4KaBl5oXZyt",
        "outputId": "0a89d538-7a98-4b04-c3e9-48c2b8010cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}